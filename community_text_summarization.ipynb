{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "b2c40d47ab21448b91e3523c8ad52510": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_dcda2cdacf994259870da31e58587d2d",
              "IPY_MODEL_43d2612ac4c6432cb6738a51253de549",
              "IPY_MODEL_8da3c79171154699b94ea8d4e35f9b92"
            ],
            "layout": "IPY_MODEL_4965a59de97d4d8082e26722d9f3c0b4"
          }
        },
        "dcda2cdacf994259870da31e58587d2d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8f99444bc1a54e27882bf7ece75639d0",
            "placeholder": "​",
            "style": "IPY_MODEL_98eba40af8114439b221aca8eaa95f40",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "43d2612ac4c6432cb6738a51253de549": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_cae863535dc148ef865904e383b6570d",
            "max": 3,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_a89b9e73db0740a183b922230a406e51",
            "value": 3
          }
        },
        "8da3c79171154699b94ea8d4e35f9b92": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9b2c63c9d5eb411a82f7cceea0da3752",
            "placeholder": "​",
            "style": "IPY_MODEL_3e239183aad741658833bcfb15bb99d4",
            "value": " 3/3 [00:02&lt;00:00,  1.03it/s]"
          }
        },
        "4965a59de97d4d8082e26722d9f3c0b4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8f99444bc1a54e27882bf7ece75639d0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "98eba40af8114439b221aca8eaa95f40": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "cae863535dc148ef865904e383b6570d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a89b9e73db0740a183b922230a406e51": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "9b2c63c9d5eb411a82f7cceea0da3752": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3e239183aad741658833bcfb15bb99d4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# 1. 필수 라이브러리 설치"
      ],
      "metadata": {
        "id": "3WODSEIAG5md"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U -q transformers langchain peft bitsandbytes trl datasets notebook accelerate evaluate"
      ],
      "metadata": {
        "id": "Pduww6WyG9GD"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain_community"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8xyQosiIHS19",
        "outputId": "6a952378-c053-4103-e80d-a5e16099244c"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting langchain_community\n",
            "  Downloading langchain_community-0.3.1-py3-none-any.whl.metadata (2.8 kB)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain_community) (6.0.2)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain_community) (2.0.35)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain_community) (3.10.8)\n",
            "Collecting dataclasses-json<0.7,>=0.5.7 (from langchain_community)\n",
            "  Downloading dataclasses_json-0.6.7-py3-none-any.whl.metadata (25 kB)\n",
            "Requirement already satisfied: langchain<0.4.0,>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from langchain_community) (0.3.1)\n",
            "Requirement already satisfied: langchain-core<0.4.0,>=0.3.6 in /usr/local/lib/python3.10/dist-packages (from langchain_community) (0.3.8)\n",
            "Requirement already satisfied: langsmith<0.2.0,>=0.1.125 in /usr/local/lib/python3.10/dist-packages (from langchain_community) (0.1.130)\n",
            "Requirement already satisfied: numpy<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain_community) (1.26.4)\n",
            "Collecting pydantic-settings<3.0.0,>=2.4.0 (from langchain_community)\n",
            "  Downloading pydantic_settings-2.5.2-py3-none-any.whl.metadata (3.5 kB)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain_community) (2.32.3)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<9.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain_community) (8.5.0)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (2.4.2)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (6.1.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.13.1)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (4.0.3)\n",
            "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7,>=0.5.7->langchain_community)\n",
            "  Downloading marshmallow-3.22.0-py3-none-any.whl.metadata (7.2 kB)\n",
            "Collecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7,>=0.5.7->langchain_community)\n",
            "  Downloading typing_inspect-0.9.0-py3-none-any.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: langchain-text-splitters<0.4.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from langchain<0.4.0,>=0.3.1->langchain_community) (0.3.0)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.10/dist-packages (from langchain<0.4.0,>=0.3.1->langchain_community) (2.9.2)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4.0,>=0.3.6->langchain_community) (1.33)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4.0,>=0.3.6->langchain_community) (24.1)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4.0,>=0.3.6->langchain_community) (4.12.2)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.2.0,>=0.1.125->langchain_community) (0.27.2)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.2.0,>=0.1.125->langchain_community) (3.10.7)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.2.0,>=0.1.125->langchain_community) (1.0.0)\n",
            "Collecting python-dotenv>=0.21.0 (from pydantic-settings<3.0.0,>=2.4.0->langchain_community)\n",
            "  Downloading python_dotenv-1.0.1-py3-none-any.whl.metadata (23 kB)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain_community) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain_community) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain_community) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain_community) (2024.8.30)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain_community) (3.1.1)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.125->langchain_community) (3.7.1)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.125->langchain_community) (1.0.6)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.125->langchain_community) (1.3.1)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.125->langchain_community) (0.14.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.10/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.4.0,>=0.3.6->langchain_community) (3.0.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain<0.4.0,>=0.3.1->langchain_community) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.23.4 in /usr/local/lib/python3.10/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain<0.4.0,>=0.3.1->langchain_community) (2.23.4)\n",
            "Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain_community)\n",
            "  Downloading mypy_extensions-1.0.0-py3-none-any.whl.metadata (1.1 kB)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.125->langchain_community) (1.2.2)\n",
            "Downloading langchain_community-0.3.1-py3-none-any.whl (2.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m43.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dataclasses_json-0.6.7-py3-none-any.whl (28 kB)\n",
            "Downloading pydantic_settings-2.5.2-py3-none-any.whl (26 kB)\n",
            "Downloading marshmallow-3.22.0-py3-none-any.whl (49 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.3/49.3 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading python_dotenv-1.0.1-py3-none-any.whl (19 kB)\n",
            "Downloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
            "Downloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
            "Installing collected packages: python-dotenv, mypy-extensions, marshmallow, typing-inspect, pydantic-settings, dataclasses-json, langchain_community\n",
            "Successfully installed dataclasses-json-0.6.7 langchain_community-0.3.1 marshmallow-3.22.0 mypy-extensions-1.0.0 pydantic-settings-2.5.2 python-dotenv-1.0.1 typing-inspect-0.9.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install bitsandbytes"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yx23Pa7iZiQ7",
        "outputId": "4298bae3-5df3-46f6-a442-ea62c091861d"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: bitsandbytes in /usr/local/lib/python3.10/dist-packages (0.44.1)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from bitsandbytes) (2.4.1+cu121)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from bitsandbytes) (1.26.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (3.16.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (2024.6.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->bitsandbytes) (2.1.5)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->bitsandbytes) (1.3.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. HTML 테이블 스타일링"
      ],
      "metadata": {
        "id": "JXVSAM57G_6_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.core.display import HTML, Markdown\n",
        "\n",
        "table_css = \"\"\"\n",
        "    table {\n",
        "        align: left; display: block\n",
        "    }\n",
        "\"\"\"\n",
        "HTML('<style>{}</style>'.format(table_css))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "gU4bYPDAHE7L",
        "outputId": "dd5e1566-f8d5-4288-f1a1-7df92facfcad"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<style>\n",
              "    table {\n",
              "        align: left; display: block\n",
              "    }\n",
              "</style>"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. 라이브러리 임포트 및 시드 설정"
      ],
      "metadata": {
        "id": "fsQ8NVuSHH_S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline, set_seed\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
        "from accelerate.utils import release_memory\n",
        "import torch\n",
        "from datasets import Dataset\n",
        "from trl import SFTTrainer\n",
        "from peft import LoraConfig, PeftModel\n",
        "import pandas as pd\n",
        "from langchain.chains.summarize import load_summarize_chain\n",
        "from langchain.text_splitter import CharacterTextSplitter, HTMLHeaderTextSplitter\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain.docstore.document import Document\n",
        "from langchain_community.llms.huggingface_pipeline import HuggingFacePipeline\n",
        "import evaluate\n",
        "import transformers\n",
        "from langchain.llms.base import LLM\n",
        "from typing import Any\n",
        "import warnings\n",
        "import gc\n",
        "import random\n",
        "import numpy as np\n",
        "\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# 재현성을 위한 시드 설정\n",
        "set_seed(42)\n",
        "torch.manual_seed(42)\n",
        "torch.cuda.manual_seed(42)\n",
        "np.random.seed(42)\n",
        "random.seed(42)\n"
      ],
      "metadata": {
        "id": "KzoJD0l0HKrL"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4. 데이터 로드"
      ],
      "metadata": {
        "id": "izfbGGbWHMuL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 데이터 로드\n",
        "Message = pd.read_csv(\"/content/SPAM text message 20170820 - Data.csv\")\n",
        "Message = Message.drop_duplicates(subset=['Category', 'Message']).reset_index(drop=True)"
      ],
      "metadata": {
        "id": "MAf9BssVHehg"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 5. 첫 번째 글 가져오기 및 미리보기"
      ],
      "metadata": {
        "id": "c4SN2IvkHdps"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 첫 번째 글 가져오기\n",
        "Message = Message.iloc[0]['Message']  # 컬럼 이름이 'writeup'이라고 가정\n",
        "print('문자 수:', len(Message))\n",
        "print(Message[:1000])  # 처음 1000자 출력\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zf9IkEnHHp5L",
        "outputId": "191ec561-348d-47b3-8880-37918f8bcbc9"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "문자 수: 111\n",
            "Go until jurong point, crazy.. Available only in bugis n great world la e buffet... Cine there got amore wat...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 6. 요약을 위한 프롬프트 생성 및 실행"
      ],
      "metadata": {
        "id": "iDjwV8QKHvAC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Hugging Face 로그인 추가\n",
        "from huggingface_hub import login\n",
        "\n",
        "# Hugging Face 토큰으로 로그인\n",
        "api_token = \"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\"\n",
        "login(api_token)\n",
        "\n",
        "# Hugging Face 파이프라인 및 필요한 라이브러리 임포트\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
        "\n",
        "quantization_config = BitsAndBytesConfig(load_in_8bit=True)\n",
        "\n",
        "# 모델과 토크나이저 불러오기\n",
        "model_name = \"google/gemma-2-2b\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
        "\n",
        "# 파이프라인을 GPU에서 실행하도록 설정 (device=0은 첫 번째 GPU 사용)\n",
        "pipe = pipeline(\"text-generation\", model=model, tokenizer=tokenizer, device=-1)\n",
        "\n",
        "# 시드 설정\n",
        "set_seed(42)\n",
        "\n",
        "# 요약할 메시지 설정\n",
        "input_text = \"요약할 텍스트를 여기에 입력하세요.\"\n",
        "messages = [\n",
        "    {\n",
        "        \"role\": \"user\",\n",
        "        \"content\": f\"다음 텍스트를 기술적인 방식으로 요약해 주세요. 사실, 숫자, 사용된 전략에 중점을 두고, 요약을 장으로 나누고, 비인격적으로 작성하며, 불릿 포인트를 사용하세요:\\n\\n{input_text}\"\n",
        "    }\n",
        "]\n",
        "\n",
        "# 입력 텍스트를 파이프라인에 전달하여 결과 생성\n",
        "outputs = pipe(\n",
        "    messages[0][\"content\"],\n",
        "    max_length=256,\n",
        "    do_sample=True,\n",
        "    temperature=0.1,\n",
        "    top_k=20,\n",
        "    top_p=0.3\n",
        ")\n",
        "\n",
        "# 결과 출력\n",
        "summary = outputs[0][\"generated_text\"]\n",
        "from IPython.display import Markdown\n",
        "display(Markdown(summary.replace('#', '')))\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 182,
          "referenced_widgets": [
            "b2c40d47ab21448b91e3523c8ad52510",
            "dcda2cdacf994259870da31e58587d2d",
            "43d2612ac4c6432cb6738a51253de549",
            "8da3c79171154699b94ea8d4e35f9b92",
            "4965a59de97d4d8082e26722d9f3c0b4",
            "8f99444bc1a54e27882bf7ece75639d0",
            "98eba40af8114439b221aca8eaa95f40",
            "cae863535dc148ef865904e383b6570d",
            "a89b9e73db0740a183b922230a406e51",
            "9b2c63c9d5eb411a82f7cceea0da3752",
            "3e239183aad741658833bcfb15bb99d4"
          ]
        },
        "id": "4f-DweLSHyJv",
        "outputId": "4d470863-4743-47a5-c34d-f514862cb249"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
            "Token is valid (permission: fineGrained).\n",
            "Your token has been saved to /root/.cache/huggingface/token\n",
            "Login successful\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "b2c40d47ab21448b91e3523c8ad52510"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "The 'max_batch_size' argument of HybridCache is deprecated and will be removed in v4.46. Use the more precisely named 'batch_size' argument instead.\n",
            "Starting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 7. HTML 헤더 기준으로 텍스트 분할"
      ],
      "metadata": {
        "id": "m5wAy5QXH0Hh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# HTML 헤더 기준으로 분할\n",
        "headers_to_split_on = [\n",
        "    (\"h1\", \"Header 1\"),\n",
        "    (\"h2\", \"Header 2\")\n",
        "]\n",
        "\n",
        "text_splitter = HTMLHeaderTextSplitter(headers_to_split_on=headers_to_split_on, return_each_element=False)\n",
        "texts_html_Message = text_splitter.split_text(Message)\n",
        "\n",
        "print('글 길이:', len(Message))\n",
        "print('분할된 개수:', len(texts_html_Message))\n",
        "print('반환된 요소 타입:', type(texts_html_Message[0]))\n",
        "print('각 분할의 길이:', [len(i.page_content) for i in texts_html_Message])\n",
        "\n",
        "# 각 분할의 처음 50자 출력\n",
        "print([(i.page_content[:50], i.metadata) for i in texts_html_Message])\n"
      ],
      "metadata": {
        "id": "qhiJJpgKH6BB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 8. 메타데이터 추가"
      ],
      "metadata": {
        "id": "45oubn6QH7qQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for i, text in enumerate(texts_html_Message):\n",
        "    # 메타데이터와 콘텐츠를 결합\n",
        "    final_content = '\\n'.join(text.metadata.values()) + '\\n' + text.page_content\n",
        "    text.page_content = final_content\n",
        "\n",
        "    # 예시 출력\n",
        "    if i < 2:\n",
        "        print(final_content)\n",
        "        print()\n"
      ],
      "metadata": {
        "id": "O8x4BH4zIACR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 9. 텍스트 청크 분할"
      ],
      "metadata": {
        "id": "WuZjLu-8ICB2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text_splitter = CharacterTextSplitter(chunk_size=2000, chunk_overlap=100)\n",
        "\n",
        "# 분할 수행\n",
        "splits = text_splitter.split_documents(texts_html_Message)\n",
        "print('최종 분할 개수:', len(splits))\n",
        "print('각 분할의 길이:', [len(i.page_content) for i in splits])\n",
        "\n",
        "# 각 분할의 처음 50자 및 메타데이터 출력\n",
        "print([(i.page_content[:50], i.metadata) for i in splits])\n"
      ],
      "metadata": {
        "id": "2PR0-ixJIEI6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 10. CUDA 메모리 정리"
      ],
      "metadata": {
        "id": "hBtyY0wiIPN6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with torch.no_grad():\n",
        "    torch.cuda.empty_cache()\n",
        "gc.collect()\n"
      ],
      "metadata": {
        "id": "uMe_LyGRISYz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 11. 커스텀 LLM 클래스 정의"
      ],
      "metadata": {
        "id": "ucgKrIpaIUYI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class GemmaLLM(LLM):\n",
        "    hf_pipe: Any = None\n",
        "    pipe_kwargs: Any = None\n",
        "\n",
        "    def __init__(self, hf_pipeline, pipe_kwargs):\n",
        "        super(GemmaLLM, self).__init__()\n",
        "        self.hf_pipe = hf_pipeline\n",
        "        self.pipe_kwargs = pipe_kwargs\n",
        "\n",
        "    @property\n",
        "    def _llm_type(self):\n",
        "        return \"Gemma pipeline\"\n",
        "\n",
        "    def _call(self, prompt, **kwargs):\n",
        "        # LangChain에서 호출 시 Hugging Face 파이프라인을 통해 텍스트 생성\n",
        "        return self.hf_pipe(prompt, **self.pipe_kwargs)\n"
      ],
      "metadata": {
        "id": "Ffkq9PRmIYDD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 12. 모델 로드 및 미세 조정"
      ],
      "metadata": {
        "id": "ACXlF419IaYx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "base_model_name = \"gpt2\"  # 사용하려는 사전 학습 모델 이름으로 변경\n",
        "adapter_model_name = \"/content/drive/MyDrive/lora_adapter\"  # LoRA 어댑터 경로\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(base_model_name, device_map='auto', torch_dtype=torch.float16)\n",
        "model = PeftModel.from_pretrained(model, adapter_model_name, device_map='auto', torch_dtype=torch.float16)\n",
        "\n",
        "# 어댑터를 베이스 모델에 통합\n",
        "model = model.merge_and_unload()\n",
        "model.save_pretrained('/content/drive/MyDrive/final_model')\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(base_model_name)\n"
      ],
      "metadata": {
        "id": "jylGaVk4Id0E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 13. 미세 조정된 모델을 파이프라인으로 로드"
      ],
      "metadata": {
        "id": "EnoAm3OzIfkm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_path = \"/content/drive/MyDrive/final_model\"\n",
        "\n",
        "pipe_finetuned = pipeline(\n",
        "    \"text-generation\",\n",
        "    model=model_path,\n",
        "    tokenizer=tokenizer,\n",
        "    model_kwargs={\"torch_dtype\": torch.float16},\n",
        "    device_map='auto',\n",
        "    max_new_tokens=512\n",
        ")\n"
      ],
      "metadata": {
        "id": "_bcDeOgCIjGH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 14. 미세 조정된 모델로 텍스트 생성"
      ],
      "metadata": {
        "id": "MeoZxOLzIlSn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "outputs = pipe_finetuned(\n",
        "    prompt,\n",
        "    do_sample=True,\n",
        "    temperature=0.1,\n",
        "    top_k=20,\n",
        "    top_p=0.3,\n",
        "    add_special_tokens=True\n",
        ")\n",
        "\n",
        "summary = pipe.tokenizer.decode(outputs[0]['generated_text'], skip_special_tokens=True)\n",
        "summary = summary[len(messages[0][\"content\"]):]  # 프롬프트 제외\n",
        "display(Markdown(summary.replace('#', '')))\n"
      ],
      "metadata": {
        "id": "XPQTqTmxIoPx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 15. 커스텀 프롬프트를 사용한 요약"
      ],
      "metadata": {
        "id": "JhtZlnGYIqpl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "messages = [\n",
        "    {\n",
        "        \"role\": \"user\",\n",
        "        \"content\": f\"다음 텍스트를 2-3문장의 짧은 요약으로 작성해 주세요:\\n\\n{writeup}\"\n",
        "    }\n",
        "]\n",
        "\n",
        "prompt = pipe.tokenizer.encode(messages[0][\"content\"], return_tensors='pt')\n",
        "\n",
        "outputs = pipe(\n",
        "    prompt,\n",
        "    max_length=150,\n",
        "    do_sample=True,\n",
        "    temperature=0.1,\n",
        "    top_k=20,\n",
        "    top_p=0.3,\n",
        "    eos_token_id=pipe.tokenizer.eos_token_id\n",
        ")\n",
        "\n",
        "summary = pipe.tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "summary = summary[len(messages[0][\"content\"]):]  # 프롬프트 제외\n",
        "display(Markdown(summary.replace('#', '')))\n"
      ],
      "metadata": {
        "id": "hcSHH-f6IuTJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 16. 미세 조정된 모델을 사용한 추가 요약"
      ],
      "metadata": {
        "id": "n4m751ktIxl1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "outputs = pipe_finetuned(\n",
        "    prompt,\n",
        "    do_sample=True,\n",
        "    temperature=0.1,\n",
        "    top_k=20,\n",
        "    top_p=0.3,\n",
        "    add_special_tokens=True\n",
        ")\n",
        "\n",
        "summary = pipe.tokenizer.decode(outputs[0]['generated_text'], skip_special_tokens=True)\n",
        "summary = summary[len(messages[0][\"content\"]):]  # 프롬프트 제외\n",
        "display(Markdown(summary.replace('#', '')))\n"
      ],
      "metadata": {
        "id": "iJKRGg78I0uE"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}